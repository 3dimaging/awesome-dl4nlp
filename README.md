Awesome Deep Learning for Natural Language Processing (NLP) [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)
====

Table of Contents
----

- __[Courses](#courses)__  

- __[Videos and Lectures](#videos-and-lectures)__  

- __[Papers](#papers)__  

- __[Blog Posts](#blog-posts)__

- __[Tutorials / Demos](#tutorials)__  

- __[Frameworks](#frameworks)__  

- __[Researchers](#researchers)__  

- __[Datasets](#datasets)__  

- __[Miscellaneous](#miscellaneous)__  

- __[Contributing](#contributing)__  

Courses
----
1. [CS224d: Deep Learning for Natural Language Processing from Stanford](http://cs224d.stanford.edu/)

Videos and Lectures
----
1. [Ali Ghodsi's lecture on word2vec, part 1](https://www.youtube.com/watch?v=TsEGsdVJjuA); [Ali Ghodsi's lecture on word2vec, part 2](https://www.youtube.com/watch?v=nuirUEmbaJU)
2. [Richard Socher's talk on sentiment analysis, question answering, and sentence-image embeddings](https://www.youtube.com/watch?v=tdLmf8t4oqM)

Papers
----
1. [Deep or shallow, NLP is breaking out](http://dl.acm.org/citation.cfm?id=2874915) General overview of how Deep Learning is impacting NLP
2. [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) The original word2vec paper
3. [word2vec Parameter Learning Explained](http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf)
4. [Distributed Representations of Sentences and Documents](http://cs.stanford.edu/~quocle/paragraph_vector.pdf)
5. [Context Dependent Recurrent Neural Network Language Model](http://www.msr-waypoint.com/pubs/176926/rnn_ctxt.pdf)
6. [Translation Modeling with Bidirectional Recurrent Neural Networks](https://www-i6.informatik.rwth-aachen.de/publications/download/936/SundermeyerMartinAlkhouliTamerWuebkerJoernNeyHermann--TranslationModelingwithBidirectionalRecurrentNeuralNetworks--2014.pdf)
7. [Contextual LSTM (CLSTM) models for Large scale NLP tasks](https://arxiv.org/abs/1602.06291)
8. [LSTM Neural Networks for Language Modeling](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.248.4448&rep=rep1&type=pdf)
9. [Exploring the Limits of Language Modeling](http://arxiv.org/pdf/1602.02410.pdf)
10. [Conversational Contextual Cues](https://arxiv.org/abs/1606.00372) Models context and participants in converstations
11. [Sequence to sequence learning with neural networks](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)
12. [Efficient Estimation of Word Representations in Vector Space](http://arxiv.org/pdf/1301.3781.pdf)
13. [Learning Character-level Representations for Part-of-Speech Tagging](http://jmlr.org/proceedings/papers/v32/santos14.pdf)
14. [Representation Learning for Text-level Discourse Parsing](http://www.cc.gatech.edu/~jeisenst/papers/ji-acl-2014.pdf)
15. [Fast and Robust Neural Network Joint Models for Statistical Machine Translation](http://acl2014.org/acl2014/P14-1/pdf/P14-1129.pdf)
16. [Parsing With Compositional Vector Grammars](http://www.socher.org/index.php/Main/ParsingWithCompositionalVectorGrammars)
17. [Smart Reply: Automated Response Suggestion for Email](https://arxiv.org/abs/1606.04870)
18. [Neural Architectures for Named Entity Recognition](https://arxiv.org/abs/1603.01360) state-of-the-art performance in NER with bidirectional LSTM with a sequential conditional
random layer and transition-based parsing with stack LSTMs.
19. [GloVe: Global Vectors for Word Representation](http://www-nlp.stanford.edu/pubs/glove.pdf) a "count-based"/co-occurrence model to learn word embeddings
20. [Grammar as a Foreign Language](https://arxiv.org/abs/1412.7449) State-of-the-art syntactic constituency parsing using generic sequence-to-sequence approach

Blog Posts
----
1. http://karpathy.github.io/2015/05/21/rnn-effectiveness/
2. http://instagram-engineering.tumblr.com/post/117889701472/emojineering-part-1-machine-learning-for-emoji
3. http://getdango.com/emoji-and-deep-learning.html

Tutorials
----
1. [Deep Learning for Natural Language Processing (without Magic)](http://www.socher.org/index.php/DeepLearningTutorial/DeepLearningTutorial)
2. [Vector Representations of Words - TensorFlow](https://www.tensorflow.org/versions/r0.8/tutorials/word2vec/index.html)
3. [Recurrent Neural Networks with Word Embeddings](http://deeplearning.net/tutorial/rnnslu.html)
4. [LSTM Networks for Sentiment Analysis](http://deeplearning.net/tutorial/lstm.html)
5. [Tensorflow demo using the Large Movie Review Dataset] (http://ai.stanford.edu/~amaas/data/sentiment/)
6. [LSTMVis: Visual Analysis for Recurrent Neural Networks](http://lstm.seas.harvard.edu/client/index.html)

Frameworks
----
1. [TensorFlow](https://www.tensorflow.org/) A cross-platform, general purpose Deep Learning library with Python and C++ API
2. [Topic modeling for humans](https://pypi.python.org/pypi/gensim) A Python package that includes word2vec and doc2vec implementations.
3. [Googleâ€™s original word2vec implementation](https://code.google.com/archive/p/word2vec/)

Researchers
----
1. [Christopher Manning](http://nlp.stanford.edu/manning/)
2. [Ali Ghodsi](https://uwaterloo.ca/data-science/)
3. [Richard Socher](http://www.socher.org/)

Datasets
----
1. [Dataset from "One Billion Word Language Modeling Benchmark"](http://www.statmt.org/lm-benchmark/1-billion-word-language-modeling-benchmark-r13output.tar.gz) Almost 1B words, already pre-processed text.

Miscellaneous
----
1. [word2vec analogy demo](http://deeplearner.fz-qqq.net/)

-----
Contributing
----
Have anything in mind that you think is awesome and would fit in this list? Feel free to send a pull request!

-----
License
----

[![CC0](http://i.creativecommons.org/p/zero/1.0/88x31.png)](http://creativecommons.org/publicdomain/zero/1.0/)

To the extent possible under law, [Dr. Brian J. Spiering](http://www.linkedin.com/in/brianspiering/) has waived all copyright and related or neighboring rights to this work.
